{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "plt.style.use(['science'])\n",
    "matplotlib.rcParams[\"font.size\"] = \"12\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layers, hidden_dim, input_dim=1, output_dim=1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        self.L = hidden_layers\n",
    "        self.W = hidden_dim\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        inp_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        out_linear = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.model.add_module('input', inp_linear)\n",
    "        self.model.add_module('activ0', self.activation)\n",
    "        for i in range(hidden_layers - 1):\n",
    "            linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.model.add_module(f'linear{i+1}', linear)\n",
    "            self.model.add_module(f'activ{i+1}', self.activation)\n",
    "        self.model.add_module('output', out_linear)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def weights_norm(self):\n",
    "        norms = []\n",
    "        with torch.no_grad():\n",
    "            for name, parameter in self.model.named_parameters():\n",
    "                norms.append(torch.linalg.norm(parameter))\n",
    "        return np.array(norms)\n",
    "                \n",
    "def rmse(predicts, target):\n",
    "    '''\n",
    "    Note that for diffusion both arrays have shape (T, X).\n",
    "    '''\n",
    "    return np.sqrt(np.square(predicts - target).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lotka-Volterra Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lotkavolterra(\n",
    "    t, predictions, numerical, title=None, figsize=(5, 5), \n",
    "    show=True, save=False, path='./plot', dpi=300\n",
    "    ):\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.plot(t, predictions[0], label=r'$\\mathcal{X}(t)$')\n",
    "    plt.plot(t, predictions[1], label=r'$\\mathcal{Y}(t)$')\n",
    "    plt.plot(t, numerical[0], label=r'$x(t)$')\n",
    "    plt.plot(t, numerical[1], label=r'$y(t)$')\n",
    "    \n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('Population size')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'{path}.png', dpi=dpi)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LotkaVolterraProblem:\n",
    "    def __init__(self, T, params, initial_conditions):\n",
    "        \n",
    "        self.T = T\n",
    "        self.alpha, self.beta, self.delta, self.gamma = params\n",
    "        self.init_vals = torch.tensor(initial_conditions)\n",
    "\n",
    "        self.numerical_solution = self._solve()\n",
    "    \n",
    "    def loss_initial(self, model):\n",
    "        zero = torch.tensor([0.], requires_grad=True)\n",
    "        x = model(zero)\n",
    "        return torch.mean(torch.square(x - self.init_vals))\n",
    "    \n",
    "    def loss_physical(self, model, t):\n",
    "        xy = model(t)\n",
    "        x = xy[:,[0]]\n",
    "        y = xy[:,[1]]\n",
    "\n",
    "        dX = grad(x, t, grad_outputs=torch.ones_like(x), create_graph=True)[0]\n",
    "        dY = grad(y, t, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "        \n",
    "        loss_dX = torch.mean(torch.square(dX - self.alpha * x + self.beta * x * y))\n",
    "        loss_dY = torch.mean(torch.square(dY - self.delta * x * y + self.gamma * y))\n",
    "        \n",
    "        return loss_dX, loss_dY\n",
    "    \n",
    "    def _solve(self):\n",
    "        def lotka_volterra(t, y, alpha, beta, delta, gamma):\n",
    "            x, y = y\n",
    "            dx_dt = alpha * x - beta * x * y\n",
    "            dy_dt = delta * x * y - gamma * y\n",
    "            return [dx_dt, dy_dt]\n",
    "\n",
    "        solution = solve_ivp(lotka_volterra, \n",
    "                             (0, self.T),\n",
    "                             self.init_vals, \n",
    "                             method='RK45',\n",
    "                             args=(self.alpha, self.beta, self.delta, self.gamma), \n",
    "                             t_eval=np.linspace(0, self.T, 128))\n",
    "        \n",
    "        return solution.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lotkavolterra(\n",
    "    problem,\n",
    "    model,\n",
    "    w1, w2, w3, num_iters, N_D, lr,\n",
    "    print_every=0, collect_every=0, save_every=0\n",
    "):\n",
    "    test_points = torch.linspace(0, problem.T, 128).reshape(-1, 1)\n",
    "    t = torch.linspace(0, problem.T, N_D, requires_grad=True).reshape(-1, 1)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses, errors = [], []\n",
    "\n",
    "    for i in range(num_iters + 1):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        L_I = problem.loss_initial(model)\n",
    "        L_X, L_Y = problem.loss_physical(model, t)\n",
    "\n",
    "        L = w1 * L_I + w2 * L_X + w3 * L_Y\n",
    "\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if collect_every > 0 and i % collect_every == 0:\n",
    "            preds = model(test_points).detach().numpy()\n",
    "            x = preds[:,0].flatten()\n",
    "            y = preds[:,1].flatten()\n",
    "\n",
    "            error = (rmse(x, problem.numerical_solution[0]) + rmse(y, problem.numerical_solution[1])) / 2\n",
    "\n",
    "            losses.append(np.array([L_I.item(), L_X.item(), L_Y.item(), L.  item()]))\n",
    "            errors.append(error)\n",
    "        \n",
    "        if print_every > 0 and i % print_every == 0:\n",
    "            print(f'Iteration {i} --- RMSE {error}')\n",
    "                \n",
    "        if save_every > 0 and i % save_every == 0:\n",
    "            plot_lotkavolterra(\n",
    "            test_points, [x, y], problem.numerical_solution, title=f'Iteration {i}', figsize=(6, 4), show=False, save=True, path=f'./training/lotkavolterra/Iteration {i}'\n",
    "        )\n",
    "    \n",
    "    return np.array(losses), np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Diffusion Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diffusion(x, t, predictions, size=(10, 5), show=True, save=False, path='./plot', dpi=300):\n",
    "    X, T = np.meshgrid(x, t)\n",
    "    fig = plt.figure(figsize=size)\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.plot_surface(X, T, predictions, cmap='viridis')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('t')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.imshow(predictions, origin='lower', aspect='auto', cmap='viridis', \n",
    "              extent=[x.min(), x.max(), t.min(), t.max()])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('t')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'{path}.png', dpi=dpi)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionProblem:\n",
    "    def __init__(self, D, boundaries, resolution, b_values):\n",
    "        self.D = D\n",
    "        self.L, self.R, self.T = boundaries\n",
    "        self.left_boundary, self.right_boundary, self.initial_values = b_values\n",
    "        \n",
    "        self.Nt, self.Nx = resolution\n",
    "        self.numerical_solution = self._solve()\n",
    "        self.t = torch.linspace(0, self.T, self.Nt)\n",
    "        self.x = torch.linspace(self.L, self.R, self.Nx)\n",
    "    \n",
    "    def loss_boundary(self, model, t_left, t_right, x_init):\n",
    "        \n",
    "        def nearest_index(array, values):\n",
    "            values = [np.abs(array - v).argmin() for v in values]\n",
    "            return values\n",
    "        \n",
    "        left_values =  self.left_boundary[nearest_index(self.t, t_left)  ]\n",
    "        right_values = self.right_boundary[nearest_index(self.t, t_right)]\n",
    "        init_values =  self.initial_values[nearest_index(self.x, x_init) ]\n",
    "        \n",
    "        left =  model(torch.vstack([torch.ones_like(t_left)  * self.L, t_left]).T)\n",
    "        right = model(torch.vstack([torch.ones_like(t_right) * self.R, t_right]).T)\n",
    "        init =  model(torch.vstack([x_init, torch.zeros_like(x_init)]).T)\n",
    "        \n",
    "        left_error = torch.square(left_values - left.flatten()).mean()\n",
    "        right_error = torch.square(right_values - right.flatten()).mean()\n",
    "        init_error = torch.square(init_values - init.flatten()).mean()\n",
    "        \n",
    "        return left_error + right_error, init_error\n",
    "    \n",
    "    def loss_physical(self, model, x, t):\n",
    "        u = model(torch.hstack([x, t]))\n",
    "        \n",
    "        ut =  grad(u,  t, grad_outputs=torch.ones_like(t), create_graph=True)[0]\n",
    "        ux =  grad(u,  x, grad_outputs=torch.ones_like(x), create_graph=True)[0]\n",
    "        uxx = grad(ux, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]\n",
    "    \n",
    "        return torch.mean(torch.square(ut - self.D * uxx))\n",
    "        \n",
    "    def _solve(self):\n",
    "        Nt, Nx = self.Nt, self.Nx\n",
    "        \n",
    "        dx = (self.R - self.L) / Nx     # Spatial step size\n",
    "        dt = self.T / Nt                # Time step size\n",
    "        alpha = self.D * dt / (2 * dx ** 2)\n",
    "\n",
    "        # Crank-Nicholson method\n",
    "        u = np.zeros((Nt, Nx))\n",
    "\n",
    "        # Initial condition\n",
    "        u[0, :] = self.initial_values\n",
    "\n",
    "        # Boundary conditions\n",
    "        u[:, 0], u[:, -1] = self.left_boundary, self.right_boundary\n",
    "\n",
    "        A = np.diag((1 + 2*alpha) * np.ones(Nx)) + np.diag(-alpha * np.ones(Nx-1), 1) + np.diag(-alpha * np.ones(Nx-1), -1)\n",
    "        B = np.diag((1 - 2*alpha) * np.ones(Nx)) + np.diag(alpha * np.ones(Nx-1), 1) + np.diag(alpha * np.ones(Nx-1), -1)\n",
    "            \n",
    "        A_reversed = np.linalg.inv(A)\n",
    "\n",
    "        for n in range(0, Nt - 1):\n",
    "            b = np.dot(B, u[n, :])\n",
    "            u[n+1, :] = A_reversed @ b\n",
    "\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion(\n",
    "    problem,\n",
    "    model,\n",
    "    N_LB, N_RB, N_I, N_D,\n",
    "    coef, lr, num_iters,\n",
    "    print_every=0, collect_every=0, save_every=0\n",
    "):\n",
    "\n",
    "    x_sampled = torch.tensor(np.random.uniform(problem.L, problem.R, N_I), dtype=torch.float32)\n",
    "    t_left_sampled =  torch.tensor(np.random.uniform(0, problem.T, N_LB), dtype=torch.float32)\n",
    "    t_right_sampled = torch.tensor(np.random.uniform(0, problem.T, N_RB), dtype=torch.float32)\n",
    "\n",
    "    x_pts = torch.tensor(np.random.uniform(problem.L, problem.R, N_D), requires_grad=True, dtype=torch.float32).reshape(-1, 1)\n",
    "    t_pts = torch.tensor(np.random.uniform(0, problem.T, N_D), requires_grad=True, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    test_points = torch.cartesian_prod(problem.x, problem.t)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses, errors = [], []\n",
    "\n",
    "    for i in range(num_iters + 1):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        L_B, L_I = problem.loss_boundary(model, t_left_sampled, t_right_sampled, x_sampled)\n",
    "        L_D = problem.loss_physical(model, x_pts, t_pts)\n",
    "\n",
    "        total = coef * (L_B + L_I) + (1 - coef) * L_D\n",
    "\n",
    "        total.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if collect_every > 0 and i % collect_every == 0:\n",
    "            preds = model(test_points).reshape(problem.Nx, problem.Nt).detach().numpy()\n",
    "                \n",
    "            error = rmse(preds.T, problem.numerical_solution)\n",
    "                \n",
    "            losses.append(np.array([L_I.item(), L_B.item(), L_D.item(), total.item()]))\n",
    "            errors.append(error)\n",
    "                \n",
    "        if print_every > 0 and i % print_every == 0:\n",
    "            print(f'Iteration {i} --- RMSE {error}')\n",
    "                \n",
    "        if save_every > 0 and i % save_every == 0:\n",
    "            plot_diffusion(problem.x, problem.t, preds.T, show=False, save=True, path=f'./training/diffusion/Iteration {i}')\n",
    "                \n",
    "    return np.array(losses), np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Different Tasks and Reference Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lotka-Volterra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь мы определяем проблему.\n",
    "T = 25\n",
    "alpha, beta, delta, gamma = 0.4, 0.1, 0.1, 0.6\n",
    "initial_conditions = [10., 5.]\n",
    "\n",
    "lv = LotkaVolterraProblem(\n",
    "    T=T, \n",
    "    params=(alpha, beta, delta, gamma), \n",
    "    initial_conditions=initial_conditions\n",
    "    )\n",
    "\n",
    "# Для данной задачи input_dim и output_dim фиксированы.\n",
    "L, W = 2, 64\n",
    "lv_model = FeedForwardNetwork(\n",
    "    hidden_layers=L, hidden_dim=W,\n",
    "    input_dim=1, output_dim=2\n",
    "    )\n",
    "\n",
    "# Можно задать правило инициализации. \n",
    "# Лучше всего работает метод zeros_\n",
    "lv_model.init_weights(torch.nn.init.zeros_)\n",
    "\n",
    "# Коэффициенты функции потерь \n",
    "w1, w2, w3 = 1, 2, 2\n",
    "\n",
    "# Количество точек коллокации и гиперпараметры\n",
    "N_D = 1024\n",
    "lr, num_iters = 1e-3, 25000\n",
    "\n",
    "losses, errors = train_lotkavolterra(\n",
    "    problem=lv, model=lv_model,\n",
    "    w1=w1, w2=w2, w3=w3, N_D=N_D,\n",
    "    lr=lr, num_iters=num_iters,\n",
    "    collect_every=num_iters # Это нужно чтобы выдало errors.\n",
    ")\n",
    "\n",
    "final_rmse = errors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_lv = [L, W, w1, w2, w3, N_D, lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Коэффициент диффузии\n",
    "D = 0.5\n",
    "\n",
    "# Левая, правая границы X, конечный момент времени T\n",
    "L, R, T = 0, 1, 0.5\n",
    "\n",
    "# Количество точек по времени и пространству. Их надо\n",
    "# выбирать тщательно, т.к. может не работать численное решение,\n",
    "# но можно не менять и всё будет нормально.\n",
    "Nt, Nx = 1000, 250\n",
    "\n",
    "# Граничные и начальные условия. Надо, чтобы в (L, 0) \n",
    "# и (R, 0) они совпадали, иначе всё сломается.\n",
    "left_boundary = right_boundary = torch.zeros(Nt)\n",
    "initial_conditions = torch.sin(2 * np.pi * torch.linspace(L, R, Nx)) ** 2\n",
    "\n",
    "diff = DiffusionProblem(\n",
    "    D, (L, R, T), (Nt, Nx),\n",
    "    (left_boundary, right_boundary, initial_conditions)\n",
    ")\n",
    "\n",
    "# Для данной задачи input_dim и output_dim тоже фиксированы.\n",
    "L, W = 2, 32\n",
    "diff_model = FeedForwardNetwork(\n",
    "    hidden_layers=L, hidden_dim=W,\n",
    "    input_dim=2, output_dim=1\n",
    "    )\n",
    "\n",
    "# Количество supervised точек на границах и \n",
    "# количество точек коллокации внутри домена.\n",
    "N_LB, N_RB, N_I, N_D = 64, 64, 128, 2048\n",
    "\n",
    "# Коэффициент для функции потерь.\n",
    "coef = 0.8\n",
    "\n",
    "# Гиперпараметры оптимизатора.\n",
    "lr, num_iters = 1e-3, 10000\n",
    "\n",
    "losses, errors = train_diffusion(\n",
    "    problem=diff, model=diff_model,\n",
    "    N_LB=N_LB, N_RB=N_RB, N_I=N_I, N_D=N_D,\n",
    "    coef=coef, lr=lr, num_iters=num_iters,\n",
    "    collect_every=num_iters # Это нужно чтобы выдало errors.\n",
    ")\n",
    "\n",
    "final_rmse = errors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_diff = [L, W, N_LB, N_RB, N_I, N_D, coef, lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_methods = [\n",
    "    (torch.nn.init.eye_, 'Eye'),\n",
    "    (torch.nn.init.ones_, 'Ones'),\n",
    "    (torch.nn.init.zeros_, 'Zeros'),\n",
    "    (torch.nn.init.normal_, 'Normal'),\n",
    "    (torch.nn.init.uniform_, 'Uniform'),\n",
    "    (torch.nn.init.orthogonal_, 'Orthogonal'),\n",
    "    (torch.nn.init.xavier_normal_, 'Xavier Normal'),\n",
    "    (torch.nn.init.xavier_uniform_, 'Xavier Uniform'),\n",
    "    (torch.nn.init.kaiming_normal_, 'Kaiming Normal'),\n",
    "    (torch.nn.init.kaiming_uniform_, 'Kaiming Uniform'),\n",
    "]\n",
    "\n",
    "num_trainings = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
