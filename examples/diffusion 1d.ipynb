{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(current_dir), '.'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import pinns\n",
    "\n",
    "# For cleaner output.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we solve time-dependent PDE problem with one spatial dimension and Dirichlet boundary conditions.\n",
    "\n",
    "Given domain $[0, T] \\times [A, B]$, and functions $f(x)$, $g_1(t)$, $g_2(t)$, find such $u(t, x)$ so that\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial t} = D \\frac{\\partial^2 u}{\\partial x^2}$$\n",
    "$$u(0, x) = f(x), \\quad u(t, A) = g_1(t), \\quad u(t, B) = g_2(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we get boundary conditions as functions, but one of the main features of machine learning is the ability to represent some function using finite training dataset. Let's assume that boundary values are given as finite collection of measurements and stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.samplers import RandomRectangularSampler, ConstantSampler, DataSampler\n",
    "\n",
    "path = './data/diffusion 1d/'\n",
    "\n",
    "''' -- CONSTRAINTS -- '''\n",
    "\n",
    "# Case 1: we load constraints in full length to memory.\n",
    "\n",
    "def get_data(path):\n",
    "    init = torch.tensor(np.load(path + 'init_data.npy'))\n",
    "    left = torch.tensor(np.load(path + 'left_data.npy'))\n",
    "    right = torch.tensor(np.load(path + 'right_data.npy'))\n",
    "    \n",
    "    return ([init[:,  :2], left[:,  :2], right[:,  :2]], \n",
    "            [init[:, [2]], left[:, [2]], right[:, [2]]])\n",
    "\n",
    "pts, data = get_data(path)\n",
    "constraints_sampler = ConstantSampler((pts, data))\n",
    "\n",
    "# Case 2: if constraints are very large, we may randomly \n",
    "# sample subsets of full data each time sampler called.\n",
    "\n",
    "# paths = [f'./data/{name}_data.npy' for name in ['init', 'left', 'right']]\n",
    "# constraints_sampler = DataSampler(paths, (16, 12, 8), 2)\n",
    "\n",
    "\n",
    "''' -- COLLOCATION -- '''\n",
    "\n",
    "# If our sampler returns tensor with shape [num_pts, num_coords], \n",
    "# we must edit our loss function so that it takes not t and x \n",
    "# separately, but z instead of shape [1024, 2]. Then we must \n",
    "# calculate gradient and use slices to take grad of specific \n",
    "# tensors, for example: ```ut = d(u, z)[:, 0]```. But it is very \n",
    "# ugly for our demonstration purposes (but much more efficient \n",
    "# when number of coordinates is big). We will instead expect \n",
    "# from sampler to return dictionary for each coordinate.\n",
    "\n",
    "domain = {'t': [0, 0.5], 'x': [0, 1]}\n",
    "collocation_sampler = RandomRectangularSampler(domain, 2048)\n",
    "\n",
    "\n",
    "''' -- VALIDATION DATA -- '''\n",
    "\n",
    "# We do not want to load all test points every time, so let's \n",
    "# write sampler that gives us either all data points or random \n",
    "# subset of them. 1024 is pretty much enough to fine precision.\n",
    "\n",
    "test_data_sampler = DataSampler(path + 'solution.npy', 1024, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to write loss function. Remember: we should return tuple of losses, so if you want, for example, multiply loss along each boundary, or multiply sum of boundary losses - you should just be sure that number of returned elements and number of loss coefficients are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.derivatives import Derivative\n",
    "\n",
    "d = Derivative(method='autograd')\n",
    "\n",
    "def loss(\n",
    "    cstr_pts, cstr_pred, cstr_vals,\n",
    "    coll_pts, coll_pred,\n",
    "    D = 0.5\n",
    "    ):\n",
    "    \n",
    "    # We do not need left and right because we are solving \n",
    "    # Dirichlet problem and we just compare predictions and \n",
    "    # solution. If we solve Cauchy or Robin problem, we \n",
    "    # need to calculate derivatives at boundary points.\n",
    "    \n",
    "    init_pts, left_pts, right_pts = cstr_pts\n",
    "    init_pred, left_pred, right_pred = cstr_pred\n",
    "    init_vals, left_vals, right_vals = cstr_vals\n",
    "    \n",
    "    t, x = coll_pts['t'], coll_pts['x']\n",
    "    \n",
    "    def initial_loss():\n",
    "        return torch.mean(torch.square(init_pred - init_vals))\n",
    "    \n",
    "    def left_loss():\n",
    "        return torch.mean(torch.square(left_pred - left_vals))\n",
    "    \n",
    "    def right_loss():\n",
    "        return torch.mean(torch.square(right_pred - right_vals))\n",
    "    \n",
    "    def pde_loss(u, t, x):\n",
    "        ut  = d(u,  t)\n",
    "        uxx = d(u,  x, orders = 2)\n",
    "        return torch.mean(torch.square(ut - D * uxx))\n",
    "    \n",
    "    return (\n",
    "        initial_loss(), \n",
    "        left_loss(), \n",
    "        right_loss(), \n",
    "        pde_loss(coll_pred, t, x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns import Trainer\n",
    "from pinns.models import FF\n",
    "from pinns.activations import Swish\n",
    "from pinns.optimizers import Adam\n",
    "from pinns.errors import l2 as metric\n",
    "\n",
    "pinn = FF([2, 64, 64, 1], activ=Swish(1))\n",
    "\n",
    "adam = Adam(pinn, lr = 1e-2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    loss,\n",
    "    pinn,\n",
    "    constraints_sampler,\n",
    "    collocation_sampler,\n",
    "    loss_coefs = [0.75, 0.75, 0.75, 0.25],\n",
    "    test_points_sampler = test_data_sampler\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    num_iters=1500,\n",
    "    optimizers=[(0, adam)],\n",
    "    validate_every=1,\n",
    "    error_metric=metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using L2 error, we need to be aware that if number of points increases, then error will be too. Because of that, we might prefer RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(metric, full = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know the shape of test data.\n",
    "Nt, Nx = 500, 750\n",
    "pts, values = test_data_sampler(full=True)\n",
    "\n",
    "pts = [\n",
    "    pts[:,0].reshape(Nx, Nt),\n",
    "    pts[:,1].reshape(Nx, Nt)\n",
    "]\n",
    "values = values.reshape(Nx, Nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(trainer.loss_history, label='Loss')\n",
    "ax.plot(range(0, trainer.iter + 1, 1), trainer.error_history, label='Error')\n",
    "ax.grid()\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "\n",
    "preds = pinn.predict(test_data_sampler(full=True)[0]).detach().reshape(Nx, Nt)\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_surface(pts[1], pts[0], preds, cmap='viridis')\n",
    "\n",
    "# We also plot constraints as red dots.\n",
    "cstr_pts, cstr_vals = constraints_sampler()\n",
    "stacked_pts = torch.cat([torch.hstack([t[:, [1]], t[:, [0]]]) for t in cstr_pts])\n",
    "stacked_vals = torch.cat(cstr_vals)\n",
    "constraints = torch.hstack([stacked_pts, stacked_vals.reshape(-1, 1)]).T\n",
    "ax.scatter3D(*constraints, color='r', s=10)\n",
    "# ax.view_init(80, -120)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
