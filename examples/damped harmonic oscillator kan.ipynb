{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(current_dir), '.'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import pinns\n",
    "\n",
    "# For cleaner output.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to solve Damped Harmonic Oscillator problem using Kolmogorov-Arnold Network. Our setting will be the same as in default DHO solution method. As you will see, it differs not that much from FF training (thanks to authors of pykan for their effort in providing KAN as pytorch module).\n",
    "\n",
    "Our library provides easy-to-use wrapper for pykan library, but you always kan define your own network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical(t):\n",
    "    return torch.exp(-0.4*t)*(4.59*torch.sin(1.96*t) + 5*torch.cos(1.96*t))\n",
    "\n",
    "T = 10              # We need to fix some computational domain.\n",
    "p = (0.2, 2.0)      # Parameters that yields interesting function.\n",
    "x0, v0 = 5.0, 7.0   # Initial values may be arbitrary.\n",
    "\n",
    "t = torch.linspace(0, T, 128)\n",
    "solution = analytical(t)\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.plot(t, solution)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.samplers import ConstantSampler, RandomSampler\n",
    "\n",
    "# Constraints (training data) sampler must return tuple (points, values).\n",
    "constraints_sampler = ConstantSampler((\n",
    "    torch.tensor([[0.]], requires_grad=True),\n",
    "    torch.tensor([x0, v0])\n",
    "))\n",
    "\n",
    "# Collocation sampler must return just tensor of shape [num_pts, coords].\n",
    "domain = {'t': [0, T]}\n",
    "collocation_sampler = RandomSampler(domain, 256, return_dict=False)\n",
    "\n",
    "# Test points sampler must return tuple (points, values) of shape [num_pts, coords].\n",
    "test_points_sampler = ConstantSampler(\n",
    "    (t.reshape(-1, 1), solution.reshape(-1, 1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.derivatives import Derivative\n",
    "\n",
    "d = Derivative(method = 'autograd')\n",
    "\n",
    "def loss(\n",
    "    cstr_pts, cstr_pred, cstr_vals,\n",
    "    coll_pts, coll_pred,\n",
    "    zeta = 0.2, omega = 2.0\n",
    "    ):\n",
    "    \n",
    "    def init_loss(x0, t0):\n",
    "        v0 = d(x0, t0)\n",
    "        return torch.mean(torch.square(torch.hstack([x0, v0]) - cstr_vals))\n",
    "\n",
    "    def ode_loss(x, t):\n",
    "        v, a = d(x, t, orders = [1, 2])\n",
    "        return torch.mean(torch.square(a + 2 * zeta * omega * v + omega**2 * x))\n",
    "    \n",
    "    losses = (\n",
    "        init_loss(cstr_pred, cstr_pts),\n",
    "        ode_loss(coll_pred, coll_pts)\n",
    "    )\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is simple version of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns import Trainer\n",
    "from pinns.models import KAN\n",
    "from pinns.optimizers import Adam\n",
    "\n",
    "pinn = KAN([1, 5, 1], type = 'efficient', grid_size = 15)\n",
    "print(f'Model has {pinn.count_parameters()} trainable parameters.')\n",
    "\n",
    "adam = Adam(pinn, lr = 1e-2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    loss,\n",
    "    pinn,\n",
    "    constraints_sampler,\n",
    "    collocation_sampler,\n",
    "    loss_coefs=[0.8, 0.2],    # Coefficients are very important.\n",
    "    test_points_sampler=test_points_sampler\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    num_iters=100,\n",
    "    optimizers=[(0, adam)],\n",
    "    validate_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classic KAN, we can employ grid refining method, as described in pykan: at some point we change number of grid points and continue training of the  same model. Interestingly, this method works not as for classic regression  problems (due to the peculiarities of PINNs training), but as described in  our paper it somewhat helps to decrease approximation error.\n",
    "\n",
    "Here is how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (grid size, how much iters)\n",
    "gridit = [(5, 250), (20, 250)]\n",
    "\n",
    "pinn = KAN([1, 5, 1], grid = 5)\n",
    "print(f'Model has {pinn.count_parameters()} trainable parameters.')\n",
    "\n",
    "adam = Adam(pinn, lr = 1e-2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    loss,\n",
    "    pinn,\n",
    "    constraints_sampler,\n",
    "    collocation_sampler,\n",
    "    loss_coefs=[0.8, 0.2],    # Coefficients are very important.\n",
    "    test_points_sampler=test_points_sampler\n",
    ")\n",
    "\n",
    "for i, (grid, it) in enumerate(gridit):\n",
    "    \n",
    "    pts, _ = test_points_sampler()\n",
    "\n",
    "    pinn = KAN([1, 5, 1], grid = grid).initialize_from_another_model(trainer.model, pts)\n",
    "    trainer.model = pinn\n",
    "        \n",
    "    adam = Adam(pinn, lr = 1e-2)\n",
    "    \n",
    "    # trainer.iter += 1\n",
    "    trainer.train(\n",
    "        num_iters = it,\n",
    "        optimizers=[(0, adam)],\n",
    "        validate_every=1,\n",
    "        show_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.errors import l2\n",
    "trainer.evaluate(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinn.model = torch.load('./very_good_model_dont_delete.pt')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "axs[0].plot(trainer.loss_history, label='Loss')\n",
    "axs[0].plot(trainer.error_history, label='L2')\n",
    "axs[0].grid()\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend()\n",
    "\n",
    "preds = pinn.predict(t.reshape(-1, 1))\n",
    "axs[1].plot(t, solution, label='Solution')\n",
    "axs[1].plot(t, preds.detach(), label='Predicts', linestyle=':')\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we kan visualize activation functions of our trained network, if we use default pykan version of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.model.plot(scale = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.model.auto_symbolic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.model.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this analytical formula is a little bit complicated. Here is exact solution:\n",
    "\n",
    "$$e^{-0.4t}(4.59 \\sin(1.96t) + 5 \\cos(1.96t))$$\n",
    "\n",
    "Let's see predictions of model with fixed activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "preds = pinn.predict(t.reshape(-1, 1))\n",
    "plt.plot(t, solution, label='Solution')\n",
    "plt.plot(t, preds.detach(), label='Predicts', linestyle=':')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, symbolic representation is broken (and also $R^2$ of some activation functions is pretty low). This happened due to one of two reasons (or both):\n",
    "\n",
    "1. There are not enough functions in symbolic library to search from. Maybe we should expand it.\n",
    "2. Model itself is not designed well enough: maybe if we increase the width of layer, or change some hyperparameters such as domain of activation functions, we will see better behavior.\n",
    "\n",
    "There is pretty much room for experimenting. Hope that our interpretation library will perform better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
