{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(current_dir), '.'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import pinns\n",
    "\n",
    "# For cleaner output.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve the folloving problem: given $x_0$ and $v_0$, find such $x(t): \\mathbb{R} \\to \\mathbb{R}$, so that\n",
    "\n",
    "$$\\frac{\\mathrm{d}^2 x}{\\mathrm{d}t^2} + \\zeta \\omega_{0} \\frac{\\mathrm{d}x}{\\mathrm{d}t} + \\omega_{0}^{2}x = 0$$\n",
    "$$x(0) = x_{0}, \\frac{\\mathrm{d}x}{\\mathrm{d}t}(0) = v_{0}$$\n",
    "\n",
    "Parameters $\\zeta$ and $\\omega_{0}$ are a physical parameters that characterize oscillation and it's damping.\n",
    "\n",
    "We consider problem with given $\\zeta = 0.2$, $\\omega_{0} = 2$ and $x_0 = 5, v_0 = 7$ on a domain $[0, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know analytical solution, so we can measure actual error of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical(t):\n",
    "    return torch.exp(-0.4*t)*(4.59*torch.sin(1.96*t) + 5*torch.cos(1.96*t))\n",
    "\n",
    "# We fix variables for clarity.\n",
    "T = 10\n",
    "zeta, omega = 0.2, 2.0\n",
    "x0, v0 = 5.0, 7.0\n",
    "\n",
    "t = torch.linspace(0, T, 128)\n",
    "solution = analytical(t)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.plot(t, solution)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define all basic building blocks that will be used to train our model.\n",
    "\n",
    "First of all, we need to build all of ours samplers. We can use predefined samplers if their signature satisfies all of our needs. Since initial values are just numbers and not functions or large arrays, it is reasonable to use ConstantSampler logic. For collocation points, we might choose RandomSampler. For test points, let's use ConstantSampler.\n",
    "\n",
    "If you want, for example, load data from disk or sample it from some function, or make signatures easier, you should define your own sampler. Also you can always redefine training logic and make your own Trainer, if it is really necessary (and it should not be so hard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.samplers import ConstantSampler, RandomRectangularSampler\n",
    "\n",
    "# Constraints sampler must return tuple of tensors \n",
    "# (points, values), each of shape [num_pts, coords].\n",
    "constraints_sampler = ConstantSampler((\n",
    "    torch.tensor([[0.]], requires_grad = True),\n",
    "    torch.tensor([x0, v0])\n",
    "))\n",
    "\n",
    "# Collocation sampler must return just tensor of shape [num_pts, coords].\n",
    "domain = {'t': [0, T]}\n",
    "collocation_sampler = RandomRectangularSampler(domain, 256, return_dict=False)\n",
    "\n",
    "# Test points sampler output must have same\n",
    "# structure as constraints sampler.\n",
    "test_points_sampler = ConstantSampler((t.view(-1, 1), solution.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to define loss function. For differentiation, let's use function d from our derivatives collection.\n",
    "\n",
    "Remember that internal logic of training must be consistent with sampler output and way in what model makes predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.derivatives import Derivative\n",
    "\n",
    "d = Derivative(method = 'autograd')\n",
    "\n",
    "def loss(\n",
    "    cstr_pts, cstr_pred, cstr_vals,\n",
    "    coll_pts, coll_pred,\n",
    "    zeta = 0.2, omega = 2.0\n",
    "    ):\n",
    "    \n",
    "    def init_loss(x0, t0):\n",
    "        v0 = d(x0, t0)\n",
    "        return torch.mean(torch.square(torch.hstack([x0, v0]) - cstr_vals))\n",
    "\n",
    "    def ode_loss(x, t):\n",
    "        v, a = d(x, t, orders = [1, 2])\n",
    "        return torch.mean(torch.square(a + 2 * zeta * omega * v + omega**2 * x))\n",
    "    \n",
    "    losses = (\n",
    "        init_loss(cstr_pred, cstr_pts),\n",
    "        ode_loss(coll_pred, coll_pts)\n",
    "    )\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can define some neural network and train it using default training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns import Trainer\n",
    "from pinns.models import FF\n",
    "from pinns.optimizers import Adam\n",
    "\n",
    "pinn = FF([1] + [64] + [1], activ=nn.Tanh(), biases=True)\n",
    "print(f'Model has {pinn.count_parameters()} trainable parameters.')\n",
    "\n",
    "adam = Adam(pinn, lr = 1e-2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    loss,\n",
    "    pinn,\n",
    "    constraints_sampler,\n",
    "    collocation_sampler,\n",
    "    loss_coefs=[0.8, 0.2],    # Coefficients are very important.\n",
    "    test_points_sampler=test_points_sampler\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    num_iters=1000,\n",
    "    optimizers=[(0, adam)],\n",
    "    validate_every=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinns.errors import l2\n",
    "print(f'L2 error of model is {trainer.evaluate(l2):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends on particular case, whether it is good value or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinn.model = torch.load('./very_good_model_dont_delete.pt')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "axs[0].plot(trainer.loss_history, label='Loss')\n",
    "axs[0].plot(range(0, trainer.iter + 1, 1), trainer.error_history, label='L2')\n",
    "axs[0].grid()\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend()\n",
    "\n",
    "preds = pinn.predict(t.reshape(-1, 1))\n",
    "axs[1].plot(t, solution, label='Solution')\n",
    "axs[1].plot(t, preds.detach(), label='Predicts', linestyle=':')\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
